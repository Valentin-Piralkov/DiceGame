For the implementation of the dice game-playing agent, I have used an algorithm that does both - value and policy. I have decided this implementation into a few small methods. 

Firstly, I have declared a policy, values, and a gamma value in the agent’s constructor. Those fields are used for the calculation of values and extraction of policy for the algorithm. The values field is a dictionary in which the keys are the different states of the game and the values are the state values. Initially, all the state values are 0 because all initial states in the value iteration should have a value of 0. A policy field is again a dictionary containing the game states as keys but the values of the dictionary are the optimal actions for the specific state. Initially, all values of the agent’s policy are (). The value of gamma is set to 0.9 in the constructor. I chose this value because it gives the best possible values for the agent’s policy. If the value is lower, the agent will be less likely to roll again because of the -1 reward for rerolling. This way the agent will always hold on to whatever the first row is. If the value of gamma is higher, the agent won’t mind the -1 reward for rerolling and will roll again forever. With the value of 0.9 for gamma, the agent will hold on to high results and will row again on bad rolls.

I have created a value iteration method that iterates over the states and actions to calculate the value of every state. The method uses a for loop to iterate over every state of the game. Then I have used another loop to iterate over every possible action for every state. I have used one more for loop to iterate over the probabilities, the next states, and the reward for taking specific action in a specific state. The equation for calculating the value of the state is probability + (reward + gamma x value of the next state). If the action is (0, 1, 2), the equation is just probability + reward because the value of the next state is none and will cause an error if not removed from the equation. However, the value of (0, 1, 2) will always be much higher than the other values because the reward for other actions is -1 and the reward for the hold action. This is game.actions[-1]. I chose to implement it this way so the algorithm may support a game with different numbers of dice, not just with 3. This is the game score, so the agent will always choose to hold after the first row. That’s why I have created a method calculateReward() which takes the score of the next action and combines it with the reward -1 of the current action. This way the value iteration compares the score of holding with the possible score of rerolling the dice. Finally, I have put everything into a while loop so it will iterate until the difference between the old and the new values is less than 310. I chose this number to be the border of convergence for the value iteration because I have tested the method and I believe the algorithm has the best performance and speed with this value. 

I have also created a policy iteration method to iterate over the extraction of a policy for the agent. It again iterates over the states, actions, and (probability, reward, game_over, next state) of the game. It calculates the value of action the same way as in the value iteration method. The difference is that, for every state, the method compares the values of all actions and chooses the optimal one, which is the one with the highest value. Then the method assigns this action to be the value of the agent policy for this specific state. Everything is put in a while loop which will iterate until there is no change in the policy.

The valueIteration() and policyIteration() methods are called in the agent’s constructor so correct values for the states and optimal actions should be calculated and assigned before the agent starts playing the game.

Finally, there is a play() method which takes a state as a parameter. Then it returns the optimal action for this state from the agent’s policy.